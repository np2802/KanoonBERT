{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/np2802/Indian-Legal-Semantic-Searcher/blob/main/dynamic%2Bstatistical_mlm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masked Language Modeling (MLM)"
      ],
      "metadata": {
        "id": "fTOTnIiWF-Vi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jiza9ooBGI6A",
        "outputId": "c0be66d9-b0c3-44f8-e2e5-813eae96beba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(tf.__version__)"
      ],
      "metadata": {
        "id": "EyKP9bHs0Dia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation of dependencies"
      ],
      "metadata": {
        "id": "cOBhf7icGGIg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms8FcLOYvE5A",
        "outputId": "69713ac4-2859-4575-c7db-54ef01b908f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install nltk\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary packages"
      ],
      "metadata": {
        "id": "CMHdbGTlGSLq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i6LDQSku51u",
        "outputId": "24622466-2f40-4ec9-a2d8-705310914149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertForMaskedLM, BertTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ],
      "metadata": {
        "id": "JKYgGlkrGYH0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnig4Ohwv1hZ"
      },
      "outputs": [],
      "source": [
        "# Function to read and tokenize text files from a directory\n",
        "def read_and_tokenize(directory):\n",
        "    sentences = []\n",
        "    for file in os.listdir(directory):\n",
        "        with open(os.path.join(directory, file), 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "            sentences.extend(sent_tokenize(text))\n",
        "    return sentences\n",
        "\n",
        "# Directory path to your dataset\n",
        "directory_path = '/content/drive/MyDrive/FYP/Dataset/trial'\n",
        "text_data = read_and_tokenize(directory_path) #contains sentences tokenized from all files in my data directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtG1oJIlwKcw",
        "outputId": "0f4c662f-9994-4a44-aa7e-54fa67673a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Initialize BERT tokenizer and model\n",
        "model_name = 'nlpaueb/Legal-bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForMaskedLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic MLM"
      ],
      "metadata": {
        "id": "u0t4zRDeGdVQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuY-J1Kxw18A"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset class for dynamic MLM\n",
        "class DynamicMLMPretrainingDataset(Dataset):\n",
        "    def __init__(self, text_data, tokenizer):\n",
        "        self.text_data = text_data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = self.find_max_len() # max length of each sentence\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_data) # length of text_data / number of sentences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      '''\n",
        "      returns tensors of masked token values and labels for the given index.\n",
        "      '''\n",
        "      text = self.text_data[idx]\n",
        "      tokens = self.tokenizer.encode(text, add_special_tokens=True)\n",
        "      tokens = tokens + [tokenizer.pad_token_id] * (self.max_length - len(tokens))\n",
        "      masked_tokens, labels = self.mask_tokens(tokens)\n",
        "      return torch.tensor(masked_tokens), torch.tensor(labels)\n",
        "\n",
        "\n",
        "    def find_max_len(self):\n",
        "      '''\n",
        "      returns max length of sentence\n",
        "      '''\n",
        "      tokenized_text_data = [self.tokenizer.encode(text, add_special_tokens=True) for text in self.text_data]\n",
        "      max_length = max(len(tokens) for tokens in tokenized_text_data)\n",
        "      print(\"Text max length : {}\".format(max_length))\n",
        "      return max_length\n",
        "\n",
        "    def mask_tokens(self, tokens, mask_ratio=0.15):\n",
        "      masked_indices = torch.rand(len(tokens)) < mask_ratio\n",
        "      masked_tokens = torch.tensor(tokens)\n",
        "      masked_tokens[masked_indices] = self.tokenizer.mask_token_id #[MASK]\n",
        "      labels = torch.tensor(tokens)\n",
        "      labels[~masked_indices] = -100\n",
        "      return masked_tokens, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wAXXm_Pw9fq",
        "outputId": "f33dc6ad-d0d4-4033-8fa9-f9d9447602b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text max length : 280\n",
            "6154\n"
          ]
        }
      ],
      "source": [
        "# Create DataLoader for batch training\n",
        "dataset = DynamicMLMPretrainingDataset(text_data, tokenizer)\n",
        "print(len(dataset))\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "4uipwNkwGu5M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt0_ap_2G0yF"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "from tqdm import tqdm\n",
        "\n",
        "epochs = 5\n",
        "best_loss = float('inf')\n",
        "loss_values = []\n",
        "model.train()\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch {epoch}/{epochs} completed\")\n",
        "    for batch_masked_tokens, batch_labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_masked_tokens, labels=batch_labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    loss_values.append(loss.item())\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} - Loss : {loss.item()}\")\n",
        "\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "        model.save_pretrained('/content/drive/MyDrive/FYP/models/dynamic_mlm_trained_model')\n",
        "        # tokenizer.save_pretrained('/content/drive/MyDrive/FYP/models/dynamic_mlm_trained_model/tokenizer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjaZ8ylpx5dn"
      },
      "outputs": [],
      "source": [
        "# Plot loss graph\n",
        "x = [i for i in range(0, len(loss_values))]\n",
        "y = loss_values\n",
        "plt.plot(x, y, marker='s', linestyle='--', color='green', label='dynamic-MLM')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Graph Over Epochs')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Testing"
      ],
      "metadata": {
        "id": "vgEoY8lRGj-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoModelForMaskedLM, AutoTokenizer, BertTokenizer\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "model_path ='/content/drive/MyDrive/FYP/models/dynamic_mlm_trained_model'\n",
        "model=AutoModelForMaskedLM.from_pretrained(model_path)\n",
        "tokenizer= BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "text = \"They held entrance [MASK] for admission to the post- graduate course\"\n",
        "pipe = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
        "print(pipe(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9cFrpNrI0_6",
        "outputId": "0717c46c-23b7-4e5e-9274-fe47c9746f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'score': 0.2805054783821106, 'token': 14912, 'token_str': 'examinations', 'sequence': 'they held entrance examinations for admission to the post - graduate course'}, {'score': 0.24797526001930237, 'token': 13869, 'token_str': 'exams', 'sequence': 'they held entrance exams for admission to the post - graduate course'}, {'score': 0.2038165032863617, 'token': 5852, 'token_str': 'tests', 'sequence': 'they held entrance tests for admission to the post - graduate course'}, {'score': 0.18846404552459717, 'token': 7749, 'token_str': 'examination', 'sequence': 'they held entrance examination for admission to the post - graduate course'}, {'score': 0.013816867023706436, 'token': 11360, 'token_str': 'exam', 'sequence': 'they held entrance exam for admission to the post - graduate course'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical MLM"
      ],
      "metadata": {
        "id": "R37qkmF71YyU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0cr48wrElTO"
      },
      "outputs": [],
      "source": [
        "# Statistical MLM\n",
        "class MLMPretrainingDataset(Dataset):\n",
        "    def __init__(self, text_data, tokenizer):\n",
        "        self.text_data = text_data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = self.find_max_len()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_data[idx]\n",
        "\n",
        "        # Tokenize the text and Padding\n",
        "        tokens = self.tokenizer.encode(text, add_special_tokens=True)\n",
        "        max_length = self.max_length\n",
        "        tokens = tokens + [self.tokenizer.pad_token_id] * (max_length - len(tokens))\n",
        "\n",
        "        # Create masked input and labels for MLM\n",
        "        masked_tokens, labels = self.mask_tokens(tokens)\n",
        "\n",
        "        return masked_tokens.clone().detach(), labels.clone().detach()\n",
        "\n",
        "    def find_max_len(self):\n",
        "        # Find max length\n",
        "        tokenized_text_data = [self.tokenizer.encode(text, add_special_tokens=True) for text in self.text_data]\n",
        "        max_length = max(len(tokens) for tokens in tokenized_text_data)\n",
        "        print(\"Text max length : {}\".format(max_length))\n",
        "        return max_length\n",
        "\n",
        "    def mask_tokens(self, tokens):\n",
        "        probability_matrix = torch.full((len(tokens),), 0.15)  # 20% chance of masking(Original BERT : 15%)\n",
        "        all_special_ids = [tokenizer.mask_token_id, tokenizer.sep_token_id, tokenizer.cls_token_id]\n",
        "        special_tokens_mask = [1 if token in all_special_ids else 0 for token in tokens]\n",
        "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "\n",
        "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "        masked_tokens = torch.tensor(tokens)\n",
        "        masked_tokens[masked_indices] = self.tokenizer.mask_token_id\n",
        "\n",
        "        labels = torch.tensor(tokens)\n",
        "        labels[~masked_indices] = -100  # Only compute loss on masked tokens\n",
        "\n",
        "        return masked_tokens, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A75G_F1bIGuw",
        "outputId": "828d353e-fdb8-40cf-a12c-0138ea949fca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text max length : 436\n"
          ]
        }
      ],
      "source": [
        "# Create DataLoader for batch training\n",
        "dataset = MLMPretrainingDataset(text_data, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training"
      ],
      "metadata": {
        "id": "6qjJ-pAWHAvc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VhwZ7nX0IKio"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "epochs = 5\n",
        "best_loss = float('inf')  # Initialize the best_loss with positive infinity\n",
        "loss_values_1 = list()\n",
        "model.train()\n",
        "for epoch in tqdm(range(epochs), desc=\"Epochs\", leave=True):\n",
        "    for batch_masked_tokens, batch_labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_masked_tokens, labels=batch_labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    loss_values_1.append(loss.item())\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} - Loss : {loss.item()}\")\n",
        "\n",
        "    # Save the MLM trained model for later use when a new best loss is achieved\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "        model.save_pretrained('/content/drive/MyDrive/FYP/models/statistical_mlm_trained_model')\n",
        "        tokenizer.save_pretrained('/content/drive/MyDrive/FYP/models/statistical_mlm_trained_model/tokenizer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0wZkBl4Jb2v"
      },
      "outputs": [],
      "source": [
        "# Print loss graph\n",
        "x = [i for i in range(0, len(loss_values_1))]\n",
        "y = loss_values_1\n",
        "# Create a line plot for loss\n",
        "plt.plot(x, y, marker='o', linestyle='-', color='blue', label='statistical-MLM')\n",
        "plt.legend()\n",
        "# Adding labels and title\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Graph Over Epochs')\n",
        "# Display the plot\n",
        "plt.grid(True)  # Add grid lines\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}